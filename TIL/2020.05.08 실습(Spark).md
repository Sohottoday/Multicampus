# 2020.05.08 실습

## Spark

#### 공용 class

```java
package sparkexam;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;

public class SparkUtils {
	public static JavaSparkContext getSparkContext(String name) {
		SparkConf conf = new SparkConf().setAppName(name).setMaster("local");
		return new JavaSparkContext(conf);
	}
}

```



### 실습 1

[ Spark 프로그래밍 실습(1) ]

HDFS의 /edudata 폴더의 product_click.log 파일을 읽고 Product Id 별 갯수를 세서 
다음과 같이 출력하는 Java 코드를 작성한다.

(p001,84)
(p002,66)
(p003,54)
(p004,50)
(p005,43)
(p006,51)
(p007,71)
(p008,80)
(p009,80)
(p010,58)
(p011,49)
(p012,39)
(p013,4)
(p014,10)
(p015,7)

파일명 :SparkLab1.java



```java
package sparkexam;

import java.util.Arrays;

import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

import scala.Tuple2;

public class SparkLab1 {

	public static void main(String[] args) {
		JavaSparkContext sc = SparkUtils.getSparkContext("lab1");
		
		sparkProduct(sc);
		
		sc.stop();

	}
    
    public static void sparkProduct(JavaSparkContext sc) {
        JavaRDD<String> rdd1 = sc.textFile("hdfs://192.168.111.120:9000/edudata/product_click.log");
        JavaRDD<String> rdd2 = rdd1.flatMap((String line) -> Arrays.asList(line.split(" ")[1]).iterator());
        
        JavaPairRDD<String, Integer> rdd3 = rdd2.mapToPair(word -> new Tuple2<Stirng, Integer>(word, 1));
        JavaPairRDD<String, Integer> rdd4 = rdd3.reduceByKey((a,b) -> a+b);
        JavaPairRDD<String, Integer> rdd5 = rdd4.sortByKey();
        
        rdd5.foreach(tupledata -> System.out.println("("+tupledata._1+","+tupledata._2+")"));
    }
}
```

---

### 실습 2

[ Spark 프로그래밍 실습(1) ]

HDFS의 /edudata 폴더의 product_click.log 파일을 읽고 Product Id 별 갯수를 세서 
다음과 같이 출력하는 Java 코드를 작성한다.

(p001,84)
(p009,80)
(p008,80)
(p007,71)
(p002,66)
(p010,58)
(p003,54)
(p006,51)
(p004,50)
(p011,49)
(p005,43)
(p012,39)
(p014,10)
(p015,7)
(p013,4)

파일명 :SparkLab2.java

```java
package sparkexam;

import java.util.Arrays;

import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

import scala.Tuple2;

public class SparkLab2 {

	public static void main(String[] args) {
		JavaSparkContext sc = SparkUtils.getSparkContext("lab1");
		
		sparkProduct(sc);
		
		sc.stop();

	}
	
    public static void sparkProduct(JavaSparkContext sc) {
        JavaRDD<String> rdd1 = sc.textFile("hdfs://192.168.111.120:9000/edudata/product_click.log");
        JavaRDD<String> rdd2 = rdd1.flatMap((String line) -> Arrays.asList(line.split(" ")[1]).iterator());
        
        JavaPairRDD<String, Integer> rdd3 = rdd2.mapToPair(word -> new Tuple2<String, Integer>(word, 1));
        JavaPairRDD<String, Integer> rdd4 = rdd3.reduceByKey((a,b)->a+b);
        
        JavaPairRDD<Integer, String> rdd5 = rdd4.mapToPair(x -> x.swap());
        JavaPairRDD<Integer, String> rdd6 = rdd5.sortByKey(false);
        JavaPairRDD<String, Integer> rdd7 = rdd6.mapToPair(x -> x.swap());
        
        rdd7.foreach(tupledata -> System.out.println("("+tupledata._1+","+tupledata._2+")"));
    }
}
```

---

### 실습 3

[ Spark 프로그래밍 실습(3) ]

HDFS의 /edudata 폴더의 tomcat_access_log.txt 파일을 읽고
어떤 시간대에 요청을 가장 많이 했는지 파악하여 
다음과 같이 출력하는 Java 코드를 작성한다.

제일 많이 요청된 시간은 XX 시간대입니다.

파일명 :SparkLab3.java

```java
package sparkexam;

import java.util.Arrays;

import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

import scala.Tuple2;

public class SparkLab3 {

	public static void main(String[] args) {
		JavaSparkContext sc = SparkUtils.getSparkContext("lab3");
		
		sparkProduct(sc);
		
		sc.stop();

	}
    
    public static void sparkProduct(JavaSparkContext sc) {
        JavaRDD<String> rdd1 = sc.textFile("hdfs://192.168.111.120:9000/edudata/tomcat_access_log.txt");
        JavaRDD<String> rdd2 = rdd1.flatMap((String line) -> Arrays.asList(line.split(" - - ")[1]).iterator());
        JavaRDD<String> rdd3 = rdd2.flatMap((String rightline) -> Arrays.asList(right.substring(13,15)).iterator());
        
        JavaPairRDD<String, Integer> rdd4 = rdd3.mapToPair(word -> new Tuple2<String, Integer>(word));
        
        JavaPairRDD<String, Integer> rdd5 = rdd4.reduceByKey((a,b) -> a+b);
        
        JavaPairRDD<Integer, String> rdd6 = rdd5.mapToPair(x -> x.swap());
        JavaPairRDD<Integer, String> rdd7 = rdd6.sortByKey(false);
        JavaPairRDD<String, Integer> rdd8 = rdd7.mapToPair(x -> x.swap());
        
        rdd.foreach(tupledata -> System.out.println(tupledata._1+"시 "+tupledata._2+"번 요청"));
        System.oput.println("제일 많이 요청된 시간은 "+rdd8.first()._1+"시 입니다.")
    }
    
```

